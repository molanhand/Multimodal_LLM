{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c219d572",
   "metadata": {},
   "source": [
    "## 数据读取\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b20784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eae554b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 6992/6992 [00:01<00:00, 4486.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 6992 images\n",
      "Labels shape: (6992, 8)\n",
      "\n",
      "Labels preview:\n",
      "     image_name                                           text_ocr  \\\n",
      "0   image_1.jpg  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n",
      "1  image_2.jpeg  The best of #10 YearChallenge! Completed in le...   \n",
      "2   image_3.JPG  Sam Thorne @Strippin ( Follow Follow Saw every...   \n",
      "3   image_4.png              10 Year Challenge - Sweet Dee Edition   \n",
      "4   image_5.png  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   \n",
      "\n",
      "                                      text_corrected      humour  \\\n",
      "0  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   hilarious   \n",
      "1  The best of #10 YearChallenge! Completed in le...   not_funny   \n",
      "2  Sam Thorne @Strippin ( Follow Follow Saw every...  very_funny   \n",
      "3              10 Year Challenge - Sweet Dee Edition  very_funny   \n",
      "4  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   hilarious   \n",
      "\n",
      "           sarcasm       offensive      motivational overall_sentiment  \n",
      "0          general   not_offensive  not_motivational     very_positive  \n",
      "1          general   not_offensive      motivational     very_positive  \n",
      "2    not_sarcastic   not_offensive  not_motivational          positive  \n",
      "3  twisted_meaning  very_offensive      motivational          positive  \n",
      "4     very_twisted  very_offensive  not_motivational           neutral  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 设置数据路径\n",
    "BASE_PATH = \"memotion_dataset_7k\"\n",
    "IMAGE_PATH = os.path.join(BASE_PATH, \"images\")\n",
    "LABELS_PATH = os.path.join(BASE_PATH, \"labels.csv\")\n",
    "\n",
    "# 读取标签数据\n",
    "df_labels = pd.read_csv(LABELS_PATH,index_col=0)\n",
    "\n",
    "# 读取图像文件\n",
    "def load_image(image_name):\n",
    "    try:\n",
    "        img_path = os.path.join(IMAGE_PATH, image_name)\n",
    "        img = Image.open(img_path)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 创建图像数据字典\n",
    "images_dict = {}\n",
    "for img_name in tqdm(df_labels['image_name'], desc=\"Loading images\"):\n",
    "    img = load_image(img_name)\n",
    "    if img is not None:\n",
    "        images_dict[img_name] = img\n",
    "\n",
    "print(f\"Successfully loaded {len(images_dict)} images\")\n",
    "print(f\"Labels shape: {df_labels.shape}\")\n",
    "# 显示标签数据的前几行\n",
    "print(\"\\nLabels preview:\")\n",
    "print(df_labels.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9bf7c",
   "metadata": {},
   "source": [
    "### 文本预处理步骤\n",
    "1. 添加必要的 NLTK 库\n",
    "2. 下载必要的资源\n",
    "3. 创建文本预处理函数\n",
    "4. 应用预处理并进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入额外的预处理库\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "579ae7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\86177\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\86177\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\86177\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\86177\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下载必要的NLTK资源\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47b734dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建预处理函数\n",
    "def preprocess_text(text):\n",
    "    # 转换为小写\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 移除 URL\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # 移除标点符号\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 分词\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    \n",
    "    # 去除停用词\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # 词形还原\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # 重新组合成文本\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f53851",
   "metadata": {},
   "source": [
    "## 仅使用文本进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c7da3",
   "metadata": {},
   "source": [
    "我将帮你创建一个使用简单机器学习模型进行情感分类的代码。我们将使用 scikit-learn 库来实现这个分类任务。这里我们主要关注 `offensive` 这个情感分类标签。\n",
    "\n",
    "这段代码实现了以下功能：\n",
    "\n",
    "1. 使用 TF-IDF 将文本转换为特征向量\n",
    "2. 对情感标签进行编码\n",
    "3. 将数据集分为训练集和测试集\n",
    "4. 使用随机森林分类器进行训练\n",
    "5. 评估模型性能并输出分类报告\n",
    "6. 展示最重要的特征词\n",
    "7. 可视化特征词的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8a0eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61eb6525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告：\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     negative       0.00      0.00      0.00       113\n",
      "      neutral       0.27      0.21      0.24       427\n",
      "     positive       0.42      0.71      0.53       612\n",
      "very_negative       0.00      0.00      0.00        33\n",
      "very_positive       0.12      0.01      0.03       214\n",
      "\n",
      "     accuracy                           0.38      1399\n",
      "    macro avg       0.16      0.19      0.16      1399\n",
      " weighted avg       0.29      0.38      0.31      1399\n",
      "\n",
      "\n",
      "最重要的10个特征：\n",
      "           feature  importance\n",
      "2860          meme    0.007201\n",
      "2289    imgflipcom    0.005922\n",
      "2663          like    0.005119\n",
      "3578  quickmemecom    0.005074\n",
      "2277            im    0.004902\n",
      "1327          dont    0.004702\n",
      "3195           one    0.004032\n",
      "2550          know    0.003837\n",
      "1751        friend    0.003411\n",
      "1844           get    0.003390\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "# 使用text_corrected作为特征\n",
    "X = df_labels['text_corrected'].fillna('').apply(preprocess_text) # 填充缺失值\n",
    "y = df_labels['overall_sentiment']\n",
    "\n",
    "# 标签编码\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# 文本特征提取\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 训练随机森林分类器\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 预测和评估\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# 打印分类报告\n",
    "print(\"分类报告：\")\n",
    "print(classification_report(y_test, y_pred, \n",
    "                          target_names=le.classes_))\n",
    "\n",
    "# 查看特征重要性\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': vectorizer.get_feature_names_out(),\n",
    "    'importance': rf_classifier.feature_importances_\n",
    "})\n",
    "print(\"\\n最重要的10个特征：\")\n",
    "print(feature_importance.nlargest(10, 'importance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094a524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hatememes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
