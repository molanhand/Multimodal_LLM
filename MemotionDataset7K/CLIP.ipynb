{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cef9ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Hatememes\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43a08a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Skipped 1 corrupt images. Valid images: 6986\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  We allow truncated images just in case\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "\n",
    "# 2) Load & Clean Data\n",
    "CSV_PATH = \"./memotion_dataset_7k/labels.csv\"\n",
    "IMAGES_DIR = \"./memotion_dataset_7k/images\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df[[\"image_name\", \"text_corrected\", \"offensive\"]].dropna(subset=[\"text_corrected\", \"offensive\"])\n",
    "df[\"text_corrected\"] = df[\"text_corrected\"].astype(str)\n",
    "df = df[df[\"text_corrected\"].str.strip() != \"\"]\n",
    "\n",
    "# Verify images, skip any fully corrupt\n",
    "bad_images = 0\n",
    "valid_indices = []\n",
    "for i, row in df.iterrows():\n",
    "    img_path = os.path.join(IMAGES_DIR, row[\"image_name\"])\n",
    "    try:\n",
    "        with Image.open(img_path) as im:\n",
    "            im.verify()\n",
    "        valid_indices.append(i)\n",
    "    except:\n",
    "        bad_images += 1\n",
    "df = df.loc[valid_indices].reset_index(drop=True)\n",
    "print(f\"Skipped {bad_images} corrupt images. Valid images: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83cc79af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_corrected",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "offensive",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c2364582-7e80-40d0-ba4b-2a9b5de0a19d",
       "rows": [
        [
         "0",
         "image_1.jpg",
         "LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIKUT TREND PLAY THE 10 YEARS CHALLENGE AT FACEBOOK imgflip.com",
         "not_offensive"
        ],
        [
         "1",
         "image_2.jpeg",
         "The best of #10 YearChallenge! Completed in less the 4 years. Kudus to @narendramodi ji 8:05 PM - 16 Jan 2019 from Mumbai  India",
         "not_offensive"
        ],
        [
         "2",
         "image_3.JPG",
         "Sam Thorne @Strippin ( Follow Follow Saw everyone posting these 2009 vs 2019 pics so here's mine 6:23 PM - 12 Jan 2019 O 636 Retweets 3 224 LIKES 65 636 3.2K",
         "not_offensive"
        ],
        [
         "3",
         "image_4.png",
         "10 Year Challenge - Sweet Dee Edition",
         "very_offensive"
        ],
        [
         "4",
         "image_5.png",
         "10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious 10 Year Challenge Memes | What is #10 Year Challenge?",
         "very_offensive"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>text_corrected</th>\n",
       "      <th>offensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_1.jpg</td>\n",
       "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
       "      <td>not_offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_2.jpeg</td>\n",
       "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
       "      <td>not_offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_3.JPG</td>\n",
       "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
       "      <td>not_offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_4.png</td>\n",
       "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
       "      <td>very_offensive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_5.png</td>\n",
       "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
       "      <td>very_offensive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name                                     text_corrected  \\\n",
       "0   image_1.jpg  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n",
       "1  image_2.jpeg  The best of #10 YearChallenge! Completed in le...   \n",
       "2   image_3.JPG  Sam Thorne @Strippin ( Follow Follow Saw every...   \n",
       "3   image_4.png              10 Year Challenge - Sweet Dee Edition   \n",
       "4   image_5.png  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   \n",
       "\n",
       "        offensive  \n",
       "0   not_offensive  \n",
       "1   not_offensive  \n",
       "2   not_offensive  \n",
       "3  very_offensive  \n",
       "4  very_offensive  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2a5484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged distribution:\n",
      "merge_offensive\n",
      "offensive        0.612081\n",
      "not_offensive    0.387919\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 3) Merge 4 Original Classes into2 (offensive, not_offensive)\n",
    "\n",
    "def merge_offensive(label):\n",
    "    if label in [\"slight\", \"very_offensive\",'hateful_offensive']:\n",
    "        return \"offensive\"\n",
    "    else:\n",
    "        return \"not_offensive\"\n",
    "# 合并标签\n",
    "df[\"merge_offensive\"] = df[\"offensive\"].apply(merge_offensive)\n",
    "label2id = {\"not_offensive\": 0, \"offensive\": 1}\n",
    "df[\"label\"] = df[\"merge_offensive\"].map(label2id)\n",
    "print(\"Merged distribution:\")\n",
    "print(df[\"merge_offensive\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc47cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 5588 699 699\n"
     ]
    }
   ],
   "source": [
    "# 4) Stratified Split\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    test_df, test_size=0.5, stratify=test_df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train/Val/Test sizes:\", len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949cbc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 12 files: 100%|██████████| 12/12 [06:25<00:00, 32.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\clip-vit-base-patch32'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下载模型\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "local_dir = \"D:/clip-vit-base-patch32\"  \n",
    "snapshot_download(repo_id=clip_model_name, local_dir=local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a451b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) CLIP Processor\n",
    "\n",
    "# We'll use the openai/clip-vit-base-patch32 variant \n",
    " \n",
    "processor = CLIPProcessor.from_pretrained(local_dir)\n",
    "# This processor will handle image transforms + text tokenization automatically\n",
    "# We'll apply it inside our custom dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39a75c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 6) Custom Dataset\n",
    "###############################################################################\n",
    "class CLIPMemotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item returns:\n",
    "      - pixel_values: The transformed image tensor\n",
    "      - input_ids, attention_mask: tokenized text\n",
    "      - label: the sentiment class\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, images_dir, processor, max_length=77):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.images_dir = images_dir\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.images_dir, row[\"image_name\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        text = row[\"text_corrected\"]\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "\n",
    "        # The CLIP processor can handle both images & text in a single call,\n",
    "        # but we'll call it separately for clarity. We'll do them in one go:\n",
    "        encoded = self.processor(\n",
    "            text=[text],\n",
    "            images=[image],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # encoded is a dict with keys: pixel_values, input_ids, attention_mask\n",
    "        # shape: (batch=1, channels/HW or tokens)\n",
    "\n",
    "        # We'll squeeze out batch=1 dimension so we can return plain tensors\n",
    "        pixel_values = encoded[\"pixel_values\"].squeeze(0)       # [3, 224, 224]\n",
    "        input_ids = encoded[\"input_ids\"].squeeze(0)             # [max_length]\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)   # [max_length]\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": label\n",
    "        }\n",
    "\n",
    "train_dataset = CLIPMemotionDataset(train_df, IMAGES_DIR, processor)\n",
    "val_dataset   = CLIPMemotionDataset(val_df,   IMAGES_DIR, processor)\n",
    "test_dataset  = CLIPMemotionDataset(test_df,  IMAGES_DIR, processor)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "658f1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 7) CLIP Classification Model\n",
    "###############################################################################\n",
    "class CLIPClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a CLIPModel and adds a small classifier for 2-class.\n",
    "    We'll:\n",
    "      - get image_embeds from model outputs\n",
    "      - get text_embeds from model outputs\n",
    "      - combine them, then pass through a small feedforward\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, num_labels=2, freeze_clip=False):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(model_name)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # Optionally freeze entire CLIP to reduce memory usage & avoid large updates\n",
    "        if freeze_clip:\n",
    "            for param in self.clip_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        embed_dim = self.clip_model.config.projection_dim * 2  # e.g., 512 + 512 = 1024\n",
    "        # Add a small classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # The CLIP forward pass:\n",
    "        # returns image_embeds, text_embeds, etc.\n",
    "        outputs = self.clip_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values\n",
    "        )\n",
    "        # outputs.image_embeds: [batch_size, projection_dim] default=512\n",
    "        # outputs.text_embeds:  [batch_size, projection_dim]\n",
    "\n",
    "        # By default, CLIPModel output embeddings are already normalized (unit sphere)\n",
    "        image_embeds = outputs.image_embeds\n",
    "        text_embeds = outputs.text_embeds\n",
    "\n",
    "        # Concatenate them for classification\n",
    "        fused = torch.cat([image_embeds, text_embeds], dim=1)  # shape: [B, 1024]\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "num_labels = 2\n",
    "model = CLIPClassifier(\n",
    "    model_name=clip_model_name, \n",
    "    num_labels=num_labels, \n",
    "    freeze_clip=False  \n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2021493f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Hatememes\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 8) Optimizer / Scheduler\n",
    "\n",
    "# Fine-tuning CLIP can be costly. We do a small LR.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "# Tried both simple scheduler and ReduceLROnPlateau\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "969d8409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Training / Evaluation Functions\n",
    "def epoch_step(model, dataloader, is_train=False):\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        logits = model(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\")\n",
    "    return avg_loss, acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f41ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Hatememes\\lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\Hatememes\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\Hatememes\\lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\Hatememes\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "  Train Loss: 0.6696 | Acc: 0.6120 | P: 0.3060 | R: 0.5000 | F1: 0.3797\n",
      "  Val   Loss: 0.6686   | Acc: 0.6123   | P: 0.3062 | R: 0.5000 | F1: 0.3798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Hatememes\\lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\Hatememes\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Anaconda\\envs\\Hatememes\\lib\\site-packages\\PIL\\Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/2\n",
      "  Train Loss: 0.6682 | Acc: 0.6120 | P: 0.3060 | R: 0.5000 | F1: 0.3797\n",
      "  Val   Loss: 0.6677   | Acc: 0.6123   | P: 0.3062 | R: 0.5000 | F1: 0.3798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Hatememes\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 10) Training Loop (with early stopping)\n",
    "\n",
    "EPOCHS = 2\n",
    "patience = 2\n",
    "best_val_loss = float(\"inf\")\n",
    "no_improve = 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc, train_prec, train_rec, train_f1 = epoch_step(model, train_loader, is_train=True)\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1 = epoch_step(model, val_loader, is_train=False)\n",
    "\n",
    "    # Step scheduler on val_loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | P: {train_prec:.4f} | R: {train_rec:.4f} | F1: {train_f1:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f}   | Acc: {val_acc:.4f}   | P: {val_prec:.4f} | R: {val_rec:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "256a6f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CLIP Test Results ---\n",
      "Test Loss: 0.6686\n",
      "Test Acc:  0.6123\n",
      "Macro Precision: 0.3062\n",
      "Macro Recall:    0.5000\n",
      "Macro F1:        0.3798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Hatememes\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# Final Test\n",
    "\n",
    "test_loss, test_acc, test_prec, test_rec, test_f1 = epoch_step(model, test_loader, is_train=False)\n",
    "print(\"\\n--- CLIP Test Results ---\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Acc:  {test_acc:.4f}\")\n",
    "print(f\"Macro Precision: {test_prec:.4f}\")\n",
    "print(f\"Macro Recall:    {test_rec:.4f}\")\n",
    "print(f\"Macro F1:        {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25dc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hatememes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
